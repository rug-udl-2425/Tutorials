{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ccb3d8e-15de-47d4-b46f-f6989e9c3b31",
   "metadata": {},
   "source": [
    "# 3.2 Contrastive Learning\n",
    "\n",
    "In this part of the tutorial we will be looking at contrastive learning. We will train a Siamese neural network to discriminate samples based on the similarity / contrast between them. A Siamese neural network is a regular neural network that takes 2 samples as input and processes them in parallel with the same set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a32db-fc69-40cd-b31f-6a855c8a2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import random as rd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import rand_score\n",
    "\n",
    "torch.manual_seed(0)\n",
    "rd.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c456e2-9c9e-466a-9684-3e100ca8fde7",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will again be using the MNIST dataset. However, we will slightly extend it. As mentioned, our network takes 2 samples as input. In our case the first sample is random, while the second sample has a 50/50 chance to be either another random sample from the same class or a random sample from another random class. In the former case the associated label is 1 since the 2 samples are similar, and in the latter case the label is 0. Although we might be cheating a little bit since we are using the labels, in practice the *similarity* can be defined in many ways. For example, in computer vision often the inputs are random crops of images and if the crops are from the same image, then they are similar. In such cases the labels are not used at all but that comes at the cost of way slower training and huge amounts of required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad77ae0-7381-476d-abab-24f4db815404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDigits(Dataset):\n",
    "    def __init__(self):\n",
    "        self.mnist = MNIST('data', transform=transforms.ToTensor(), download=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        # Get the first sample as usual\n",
    "        image, label = self.mnist[idx]\n",
    "\n",
    "        # 50/50 chance to get a similar 2nd sample\n",
    "        similarity_label = rd.random() > .5\n",
    "        \n",
    "        # Filter the samples based on the similarity label\n",
    "        if similarity_label:\n",
    "            other_indices = self.mnist.targets == label\n",
    "        else:\n",
    "            other_indices = self.mnist.targets != label\n",
    "\n",
    "        # Get the second random sample\n",
    "        other_idx = rd.choice(other_indices.nonzero()[:, 0])\n",
    "        other, other_label = self.mnist[other_idx]\n",
    "        \n",
    "        return image, other, similarity_label * 1. # Multply by 1. to convert True -> 1. and False -> 0.\n",
    "\n",
    "dataset = PairedDigits()ititss\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea3d30-8c04-400a-a739-7f5c0eb01aaa",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our model is a regular feed forward neural network that embeds the input images into the laten space. In our case, the latent space will be 2-dimensional so that we can visualize it. Keep in mind that this is a 99.75% reduction (28 x 28 -> 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea517b1-7e5c-434e-b488-6c4fc56ceafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "       for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for X1, X2, y in tqdm(dataloader):\n",
    "\n",
    "        # Flatten the images from [batch_size, 1, 28, 28] -> [batch_size, 1*28*28]\n",
    "        X1, X2 = X1.flatten(1), X2.flatten(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y1, y2 = model(X1, X2)\n",
    "        loss = contrastive_loss(y1, y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Loss: {total_loss/len(dataloader):.4f}\") \n",
    "    def forward(self, image: torch.Tensor, other: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # Do the forward pass for both samples\n",
    "        return (\n",
    "            self.encoder(image),\n",
    "            self.encoder(other)\n",
    "        )\n",
    "\n",
    "model = ContrastiveModel()\n",
    "optimizer = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888ade0-e9f1-48ad-a304-edfa06e82322",
   "metadata": {},
   "source": [
    "Now we can define out contrastive loss. Given 2 embeddings $x_1\\in\\mathbb{R}^q$, $x_2\\in\\mathbb{R}^q$, and the binary similarity label $y\\in\n",
    "\\{0,1\\}$, the contrastive loss is defined as follows:\n",
    "\n",
    "$$\\ell(x_1,x_2,y) = y\\frac{h(x_1,x_2)}{2} + (1-y)\\frac{\\max\\{0,m-h(x_1,x_2)\\}}{2}$$\n",
    "\n",
    "Here $m$ is a hyperparameter, the 'margin'. We will use $m=1$, feel free to change it and see what happens. $h:\\mathbb{R}^q\\times \\mathbb{R}^q\\to\\mathbb{R}$ is a distance/dissimilarity metric. Most commonly, $h(x_1,x_2)$ is the squared Euclidean distance between $x_1$ and $x_2$, i.e. $h(x_1,x_2)=||x_1-x_2||_2^2$. Small side quest: Why is calculating the square of the distance faster than calculating the distance itself?\n",
    "\n",
    "Intuitively, the loss function embodies 2 cases:\n",
    "\n",
    "- $y=1$. In this case only the first term is non-zero. Since the samples are 'similar', we aim to minimize the distance between them, so the loss is proportional to $h$.\n",
    "- $y=0$. In this case only the second term is non-zero. Since the samples are 'dissimilar', we aim to maximize the distance, so we have negative $h$. The margin $m$ is used to bind the embedding since two samples can be repelled forever ($h\\to\\infty$). We are basically saying that if the 2 samples are $m$ distance apart, they are already far enough, so the loss is 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16121a99-0f69-4b70-9c71-183e444d58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(x1: torch.Tensor, x2: torch.Tensor, y: torch.Tensor, m: float = 1.) -> torch.Tensor:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191d835-9445-4859-a8fa-8d51ff3fe727",
   "metadata": {},
   "source": [
    "The training procedure is the same as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3249028-c982-4761-b3da-73a8c98e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for X1, X2, y in tqdm(dataloader):\n",
    "\n",
    "        # Flatten the images from [batch_size, 1, 28, 28] -> [batch_size, 1*28*28]\n",
    "        X1, X2 = X1.flatten(1), X2.flatten(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y1, y2 = model(X1, X2)\n",
    "        loss = contrastive_loss(y1, y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6fb78-4056-4315-ab66-1db070438a49",
   "metadata": {},
   "source": [
    "Our model is definitely learning!\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "To visualize the latent space, we simply need to embed all training images using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f780d3a-1cd6-4c79-897f-88546f439ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new dataloader for the mnist dataset only\n",
    "mnist_dataloader = DataLoader(dataset.mnist, batch_size=100)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    predictions, labels = zip(*[(model.encoder(X.flatten(1)), y) for X, y in mnist_dataloader])\n",
    "    predictions = torch.cat(predictions)\n",
    "    labels = torch.cat(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f5285-ddc7-4746-a8c0-30ffc0eda91c",
   "metadata": {},
   "source": [
    "Now we can just plot the samples in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a58ad-137c-45a0-87e0-dd3c03d461b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=predictions[:, 0], y=predictions[:, 1], hue=labels, palette='tab10')\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.title('Contrastive Model Embedding Space')\n",
    "sns.move_legend(plt.gca(), \"upper left\", bbox_to_anchor=(1, 1), title='Digit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729269aa-90db-4ed9-9079-e56300f50dac",
   "metadata": {},
   "source": [
    "Although it seems like the samples are all over the place, they are still clustered in very tight clusters, it's just that the plot above shows all 60 000 samples. To confirm that, we can plot the densities of the clusters. We can also notice that the embedding is quite tight as it is roughly contained in the circle with center (0,0) and radius $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825bdb7-86f3-4990-9326-3d701097177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x=predictions[:, 0], y=predictions[:, 1], hue=labels, palette='tab10')\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.title('Contrastive Model Embedding Space - Density')\n",
    "sns.move_legend(plt.gca(), \"upper left\", bbox_to_anchor=(1, 1), title='Digit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e33c-f28c-4418-9247-7471905c92e9",
   "metadata": {},
   "source": [
    "Indeed the clusters are very far apart except for some pairs that did not manage to separate. It might be because we are working in only 2 dimensions so it is quite hard to fit all samples in the small circle. Still quite good! We can quantitatively evaluate the resulting clustering by fitting a model on the embedding space. Since the cluster appear to have almost circular densities, we can use Kmeans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa1d63-3db1-4564-81d6-77689fe804a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(10) # We have 10 digits\n",
    "kmeans.fit(predictions)\n",
    "cluster_assignments = kmeans.predict(predictions)\n",
    "rand_score(labels, cluster_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb5316-3637-45ff-bf6b-77a12f3cc170",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "We get a Rand Index of 0.95, which is the probability that the two clusterings (in our case the ground truth labels and the learned cluster assignments) agree on a given random sample. This is very good! Contrastive learning has been quite effective. We can further examine our clustering using the Davies-Bouldin index. To calculate it we define the following quantities based on our $n$ clusters:\n",
    "\n",
    "- Within-Cluster Distances $\\text{WCD}\\in\\mathbb{R}^n$, where $\\text{WCD}_i$ is the mean distance between all points assigned to cluster $i$ and the centroid of cluster $i$.\n",
    "- Between-Cluster Distances $\\text{BCD}\\in\\mathbb{R}^n\\times\\mathbb{R}^n$, where $\\text{BCD}_{ij}$ is the distance between the centroids of clusters $i$ and $j$.\n",
    "- The *similariy* between clusters $i$ and $j$ can now be defined as $S(i,j)=\\frac{\\text{WCD}_i+\\text{WCD}_j}{\\text{BCD}_{ij}}$\n",
    "\n",
    "Now we can define the Davies-Bouldin index as follows:\n",
    "$$\\text{Davies-Bouldin}=\\frac{1}{n}\\sum_{i=1}^n\\max_{j\\neq i}S(i,j)$$\n",
    "\n",
    "Intuitively, the index is the mean similarity between each cluster and the most similar other cluster. We aim to have maximally dissimilar clusters, so the optimal Davies-Bouldin index is 0. Go ahead and implement it to check the performance of our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
